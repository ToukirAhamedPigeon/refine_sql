import os
import re
import json
import shutil
import datetime
from pathlib import Path

# --- Utility: Logging with timestamps and durations ---
def log_step_start(step_name):
    start_time = datetime.datetime.now()
    print(f"[{start_time.strftime('%Y-%m-%d %H:%M:%S')}] ðŸš€ Starting: {step_name}")
    return start_time

def log_step_end(step_name, start_time):
    end_time = datetime.datetime.now()
    elapsed = end_time - start_time
    minutes, seconds = divmod(elapsed.total_seconds(), 60)
    print(f"[{end_time.strftime('%Y-%m-%d %H:%M:%S')}] âœ… Finished: {step_name} "
          f"(took {int(minutes)}m {int(seconds)}s)\n")

# --- Configuration ---
input_file = 'sqls/building_database.sql'
output_dir = Path("chunks")
final_output_dir = Path("results")
final_output_dir.mkdir(parents=True, exist_ok=True)

base_name = Path(input_file).stem
create_file = output_dir / f"{base_name}_0.sql"
insert_temp_file = output_dir / f"{base_name}_temp.sql"
extra_file = output_dir / f"{base_name}_temp_extra.sql"
json_index_file = output_dir / f"{base_name}_indexes.json"

refined_file = output_dir / f"{base_name}_temp_refined.sql"
final_output_file = final_output_dir / f"{base_name}.sql"
output_dir.mkdir(parents=True, exist_ok=True)

# --- Helpers for CREATE TABLE parsing ---
def adjust_varchar_and_key_length(line: str) -> tuple[str, int | None]:
    varchar_length = None
    match = re.search(r'`(\w+)`\s+varchar\((\d+)\)', line, re.IGNORECASE)
    if match:
        length = int(match.group(2))
        new_length = 100 if length <= 100 else 191
        varchar_length = new_length
        line = re.sub(r'varchar\(\d+\)', f'VARCHAR({new_length})', line, flags=re.IGNORECASE)
    line = re.sub(r'(`\w+`)\(\d+\)', r'\1(191)', line)
    return line, varchar_length

def parse_enum_values(dtype: str):
    match = re.match(r"enum\((.*)\)", dtype, re.IGNORECASE)
    if not match:
        return None
    vals = [v.strip().strip("'").strip('"') for v in match.group(1).split(',')]
    return vals

def parse_column_definition(line: str, varchar_length: int | None):
    col_def = line.strip().rstrip(',').lower()
    if not col_def or col_def.startswith(('primary', 'unique', 'key', 'constraint', 'index', ')')):
        return None
    match = re.match(r'`?(\w+)`?\s+([a-z]+(\([^)]+\))?)', col_def)
    if not match:
        return None
    col_name = match.group(1)
    dtype = match.group(2)
    nullable = 'not null' not in col_def
    default_match = re.search(r'default\s+((\'[^\']*?\')|("[^"]*?")|[^,\s]+)', col_def, re.IGNORECASE)
    default_value = default_match.group(1) if default_match else None
    enum_values = parse_enum_values(dtype)
    return {
        "name": col_name,
        "type": dtype,
        "nullable": nullable,
        "default": default_value,
        "index": None,
        "varchar_length": varchar_length,
        "enum_values": enum_values
    }

# --- Step 1: Extract CREATE TABLEs and metadata ---
step1_start = log_step_start("Step 1: Extract CREATE TABLEs and metadata")
column_metadata_map = {}
inside_create = False
create_block = []
current_table = None
column_index = 0

with open(input_file, 'r', encoding='utf-8') as fin, \
     open(create_file, 'w', encoding='utf-8') as fcreate, \
     open(extra_file, 'w', encoding='utf-8') as fextra:

    for line in fin:
        stripped = line.strip()

        if stripped.lower().startswith("create table"):
            inside_create = True
            create_block = [line]
            column_index = 0
            table_match = re.search(r"create\s+table\s+[`\"]?(\w+)[`\"]?", stripped, re.IGNORECASE)
            if table_match:
                current_table = table_match.group(1)
                column_metadata_map[current_table] = []
            continue

        if stripped.lower().startswith(("create algorithm","create view", "create trigger", "create procedure", "create function")):
            block = [line]
            while not stripped.endswith(";"):
                line = next(fin)
                stripped = line.strip()
                block.append(line)
            fextra.writelines(block)
            fextra.write("\n")
            continue

        if inside_create:
            adjusted_line, varchar_length = adjust_varchar_and_key_length(line)
            create_block.append(adjusted_line)

            if current_table:
                col_meta = parse_column_definition(adjusted_line, varchar_length)
                if col_meta:
                    col_meta["index"] = column_index
                    column_metadata_map[current_table].append(col_meta)
                    column_index += 1

            if stripped.endswith(';'):
                fcreate.writelines(create_block)
                fcreate.write("\n")
                inside_create = False
                current_table = None

with open(json_index_file, 'w', encoding='utf-8') as fjson:
    json.dump(column_metadata_map, fjson, indent=2)

log_step_end("Step 1: Extract CREATE TABLEs and metadata", step1_start)

# --- Step 2: Extract INSERT statements ---
step2_start = log_step_start("Step 2: Extract INSERT statements")
def write_insert_statements(src_path: str, dst_path: str, extra_path: str):
    with open(src_path, 'r', encoding='utf-8') as f:
        s = f.read()

    i, n = 0, len(s)
    in_single = in_double = escape = False
    start = None

    def is_insert_at(pos: int) -> bool:
        return s[pos:pos+11].lower() == 'insert into'

    with open(dst_path, 'w', encoding='utf-8') as out, \
         open(extra_path, 'a', encoding='utf-8') as fextra:

        while i < n:
            ch = s[i]
            if start is None:
                if is_insert_at(i):
                    start = i
                    i += 11
                    continue
                i += 1
                continue
            if escape:
                escape = False
            else:
                if ch == '\\' and (in_single or in_double):
                    escape = True
                elif ch == "'" and not in_double:
                    in_single = not in_single
                elif ch == '"' and not in_single:
                    in_double = not in_double
            if ch == ';' and not in_single and not in_double:
                stmt = s[start:i+1]
                insert_match = re.match(r"insert\s+into\s+[`\"]?(\w+)[`\"]?", stmt.strip(), re.IGNORECASE)
                if insert_match:
                    table_name = insert_match.group(1)
                    if table_name.lower().startswith("v_"):
                        commented_stmt = "-- " + stmt.replace("\n", "\n-- ")
                        fextra.write(commented_stmt + "\n")
                    else:
                        out.write(stmt.strip() + "\n")
                else:
                    commented_stmt = "-- " + stmt.replace("\n", "\n-- ")
                    fextra.write(commented_stmt + "\n")
                start = None
            i += 1

write_insert_statements(input_file, insert_temp_file, extra_file)
log_step_end("Step 2: Extract INSERT statements", step2_start)

# --- Step 3: Refine INSERT values ---
step3_start = log_step_start("Step 3: Refine INSERT values")
with open(json_index_file, 'r', encoding='utf-8') as f:
    table_metadata = json.load(f)

def get_fallback(dtype, enum_values=None):
    dtype = dtype.lower()
    if enum_values:
        return f"'{enum_values[0]}'"
    if dtype.startswith('int'):
        return "'0'"
    if 'float' in dtype or 'double' in dtype or 'decimal' in dtype:
        return "'0.0'"
    if dtype.startswith('date'):
        return "'1970-01-01'"
    if dtype.startswith('datetime') or dtype.startswith('timestamp'):
        return "'1970-01-01 00:00:00'"
    return "''"

def sanitize_value(inner: str) -> str:
    return re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', inner)

def truncate_value(value, col):
    max_len = col.get('varchar_length')
    enum_values = col.get('enum_values')
    is_quoted = (value.startswith("'") and value.endswith("'")) or (value.startswith('"') and value.endswith('"'))
    if is_quoted:
        inner = value[1:-1]
        inner = sanitize_value(inner)
        if enum_values and inner not in enum_values:
            inner = enum_values[0]
        if max_len:
            inner_bytes = inner.encode('utf-8')
            if len(inner_bytes) > max_len:
                truncated_bytes = inner_bytes[:max_len]
                while True:
                    try:
                        inner = truncated_bytes.decode('utf-8')
                        break
                    except UnicodeDecodeError:
                        truncated_bytes = truncated_bytes[:-1]
        inner_fixed = inner.replace("\\", "\\\\").replace("'", "''")
        return f"'{inner_fixed}'"
    return value

def fix_row(row_line, column_info):
    row = row_line.strip()
    if not row.startswith('('):
        return row_line
    ends_with_comma = row.endswith(',') or row.endswith(',\n')
    ends_with_semicolon = row.endswith(');')
    if ends_with_comma: row = row[:-1]
    elif ends_with_semicolon: row = row[:-2]
    row = row.strip().lstrip('(').rstrip(')')
    parts, current, in_quotes, escape, quote_char = [], '', False, False, ''
    for char in row:
        if escape:
            current += "\\" + char
            escape = False
        elif char == '\\':
            escape = True
        elif char in ["'", '"']:
            current += char
            if not in_quotes:
                in_quotes = True; quote_char = char
            elif char == quote_char:
                in_quotes = False
        elif char == ',' and not in_quotes:
            parts.append(current.strip()); current = ''
        else:
            current += char
    parts.append(current.strip())
    for col in column_info:
        idx = col['index']
        if idx >= len(parts): continue
        val = parts[idx]
        if val in ["''", '""']:
            if col['nullable']: parts[idx] = 'NULL'
            elif col.get('default') is not None: parts[idx] = col['default']
            else: parts[idx] = get_fallback(col['type'], col.get('enum_values'))
        else:
            parts[idx] = truncate_value(val, col)
    fixed_row = '(' + ', '.join(parts)
    if ends_with_comma: fixed_row += '),'
    elif ends_with_semicolon: fixed_row += ');'
    else: fixed_row += ')'
    return fixed_row + '\n'

with open(insert_temp_file, 'r', encoding='utf-8') as infile, open(refined_file, 'w', encoding='utf-8') as outfile:
    current_table, current_columns, row_buffer, accumulating_row = None, [], [], False
    for line in infile:
        stripped = line.strip()
        if stripped.lower().startswith('insert into'):
            if row_buffer:
                full_row = ''.join(row_buffer)
                outfile.write(fix_row(full_row, current_columns))
                row_buffer = []; accumulating_row = False
            match = re.match(r'insert into [`"]?(\w+)[`"]?\s+values', stripped, re.IGNORECASE)
            if match:
                current_table = match.group(1)
                current_columns = table_metadata.get(current_table, [])
            outfile.write(line)
        elif stripped.startswith('(') or accumulating_row:
            row_buffer.append(line); accumulating_row = True
            if stripped.endswith('),') or stripped.endswith(');'):
                full_row = ''.join(row_buffer)
                outfile.write(fix_row(full_row, current_columns))
                row_buffer = []; accumulating_row = False
        else:
            outfile.write(line)
    if row_buffer:
        full_row = ''.join(row_buffer)
        outfile.write(fix_row(full_row, current_columns))

log_step_end("Step 3: Refine INSERT values", step3_start)

# --- Step 4: Concatenate into final SQL ---
step4_start = log_step_start("Step 4: Concatenate into final SQL")
with open(final_output_file, 'w', encoding='utf-8') as fout:
    for fpath in [create_file, refined_file, extra_file]:
        if os.path.exists(fpath):
            with open(fpath, 'r', encoding='utf-8') as fin:
                fout.write(f"-- Start of {os.path.basename(fpath)}\n")
                fout.write(fin.read())
                fout.write(f"\n-- End of {os.path.basename(fpath)}\n\n")
print(f"âœ… Final concatenated SQL â†’ {final_output_file}")
log_step_end("Step 4: Concatenate into final SQL", step4_start)

# --- Step 5: Cleanup ---
step5_start = log_step_start("Step 5: Cleanup")
shutil.rmtree(output_dir, ignore_errors=True)
print("ðŸ§¹ Cleanup complete â€” only results/building_database.sql kept.")
log_step_end("Step 5: Cleanup", step5_start)
